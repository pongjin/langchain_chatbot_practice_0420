import os
import tempfile
import streamlit as st
import pandas as pd

from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.runnables import RunnableMap

import hashlib
import shutil

# âœ… íŒŒì¼ í•´ì‹œ ìƒì„±
def get_file_hash(uploaded_file):
    file_content = uploaded_file.read()
    uploaded_file.seek(0)
    return hashlib.md5(file_content).hexdigest()

# âœ… pysqlite3 íŒ¨ì¹˜
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from langchain_chroma import Chroma
os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']

st.header("(ì£¼ë§ì„ ìˆœì‚­ì‹œí‚¨)ìœ ì € ì‘ë‹µ ê¸°ë°˜ Q&A ì±—ë´‡ ğŸ’¬")

option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-4.1-nano"))

# âœ… CSV ë¡œë”© â†’ ìœ ì € ë‹¨ìœ„ë¡œ ë¬¸ì„œ ìƒì„±
@st.cache_resource
def load_csv_and_create_docs(file_path: str):
    df = pd.read_csv(file_path)

    if 'user_id' not in df.columns or 'answer' not in df.columns:
        st.error("CSV íŒŒì¼ì€ 'user_id' ì™€ 'answer' ì»¬ëŸ¼ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
        return []

    docs = []
    for user_id, group in df.groupby('user_id'):
        content = "\n".join(group['answer'].astype(str).tolist())
        metadata = {"source": f"user_{user_id}"}
        docs.append(Document(page_content=content, metadata=metadata))

    return docs

# âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def create_vector_store(file_path: str):
    docs = load_csv_and_create_docs(file_path)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    file_hash = os.path.splitext(os.path.basename(file_path))[0]
    persist_dir = f"./chroma_db_user/{file_hash}"
    if os.path.exists(persist_dir):
        shutil.rmtree(persist_dir)

    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=persist_dir
    )
    return vectorstore

# âœ… RAG ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_components(file_path: str, selected_model: str):
    vectorstore = create_vector_store(file_path)
    retriever = vectorstore.as_retriever()

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°˜ì˜í•´ í˜„ì¬ ì§ˆë¬¸ì„ ë…ë¦½í˜• ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì¤˜."),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë¬´ì¡°ê±´ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜. ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ 'ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì¤˜. ê¼­ ì´ëª¨ì§€ ì¨ì¤˜! ì°¸ê³  ë¬¸ì„œëŠ” ì•„ë˜ì— ë³´ì—¬ì¤„ ê±°ì•¼.\n\n{context}"),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = RunnableMap({
        "context": lambda x: x["context"],  # context ê·¸ëŒ€ë¡œ pass
        "answer": create_stuff_documents_chain(llm, qa_prompt)
    })

    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain

# âœ… íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (user_id, answer í¬í•¨)", type=["csv"])

if uploaded_file:
    file_hash = get_file_hash(uploaded_file)
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, f"{file_hash}.csv")

    with open(temp_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    rag_chain = initialize_components(temp_path, option)
    chat_history = StreamlitChatMessageHistory(key="chat_messages_user")

    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: chat_history,
        input_messages_key="input",
        history_messages_key="history",
        output_messages_key="answer",
    )

    if len(chat_history.messages) == 0:
        chat_history.add_ai_message("ì—…ë¡œë“œëœ ìœ ì € ì‘ë‹µ ê¸°ë°˜ìœ¼ë¡œ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¤—")

    for msg in chat_history.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("human").write(prompt_message)
        with st.chat_message("ai"):
            with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
                config = {"configurable": {"session_id": "user_session"}}
                response = conversational_rag_chain.invoke(
                    {"input": prompt_message},
                    config,
                )
                answer = response['answer']
                st.write(answer)

                if "ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤" not in answer and response.get("context"):
                    with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                        for doc in response['context']:
                            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                            source_filename = os.path.basename(source)
                            st.markdown(f"ğŸ‘¤ {source_filename}")
                            st.markdown(doc.page_content)
